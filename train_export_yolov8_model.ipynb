{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e27dd71-2b43-48bf-9f05-eba66c1afe50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.2.26  Python-3.12.2 torch-2.3.0+cpu \n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Invalid CUDA 'device=0' requested. Use 'device=cpu' or pass valid CUDA device(s) if available, i.e. 'device=0' or 'device=0,1,2,3' for Multi-GPU.\n\ntorch.cuda.is_available(): False\ntorch.cuda.device_count(): 0\nos.environ['CUDA_VISIBLE_DEVICES']: 0\nSee https://pytorch.org/get-started/locally/ for up-to-date torch install instructions if no CUDA devices are seen by torch.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m YOLO(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myolov8n.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# load a pretrained model (recommended for training)\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Train the model with 2 GPUs\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata.yaml\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimgsz\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m640\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\ultralytics\\engine\\model.py:655\u001b[0m, in \u001b[0;36mModel.train\u001b[1;34m(self, trainer, **kwargs)\u001b[0m\n\u001b[0;32m    652\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresume\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    653\u001b[0m     args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresume\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mckpt_path\n\u001b[1;32m--> 655\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer \u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_smart_load\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrainer\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43moverrides\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_callbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    656\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresume\u001b[39m\u001b[38;5;124m\"\u001b[39m):  \u001b[38;5;66;03m# manually set model only if not resuming\u001b[39;00m\n\u001b[0;32m    657\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mget_model(weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mckpt \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, cfg\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39myaml)\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\ultralytics\\engine\\trainer.py:101\u001b[0m, in \u001b[0;36mBaseTrainer.__init__\u001b[1;34m(self, cfg, overrides, _callbacks)\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs \u001b[38;5;241m=\u001b[39m get_cfg(cfg, overrides)\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_resume(overrides)\n\u001b[1;32m--> 101\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m=\u001b[39m \u001b[43mselect_device\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalidator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\ultralytics\\utils\\torch_utils.py:136\u001b[0m, in \u001b[0;36mselect_device\u001b[1;34m(device, batch, newline, verbose)\u001b[0m\n\u001b[0;32m    129\u001b[0m         LOGGER\u001b[38;5;241m.\u001b[39minfo(s)\n\u001b[0;32m    130\u001b[0m         install \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    131\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSee https://pytorch.org/get-started/locally/ for up-to-date torch install instructions if no \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    132\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA devices are seen by torch.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    133\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice_count() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    134\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    135\u001b[0m         )\n\u001b[1;32m--> 136\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    137\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid CUDA \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m requested.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    138\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Use \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice=cpu\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or pass valid CUDA device(s) if available,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    139\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m i.e. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice=0\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice=0,1,2,3\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for Multi-GPU.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    140\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mtorch.cuda.is_available(): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtorch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    141\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mtorch.cuda.device_count(): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtorch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice_count()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    142\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mos.environ[\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCUDA_VISIBLE_DEVICES\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m]: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvisible\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    143\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00minstall\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    144\u001b[0m         )\n\u001b[0;32m    146\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m cpu \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mps \u001b[38;5;129;01mand\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available():  \u001b[38;5;66;03m# prefer GPU if available\u001b[39;00m\n\u001b[0;32m    147\u001b[0m     devices \u001b[38;5;241m=\u001b[39m device\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# range(torch.cuda.device_count())  # i.e. 0,1,6,7\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: Invalid CUDA 'device=0' requested. Use 'device=cpu' or pass valid CUDA device(s) if available, i.e. 'device=0' or 'device=0,1,2,3' for Multi-GPU.\n\ntorch.cuda.is_available(): False\ntorch.cuda.device_count(): 0\nos.environ['CUDA_VISIBLE_DEVICES']: 0\nSee https://pytorch.org/get-started/locally/ for up-to-date torch install instructions if no CUDA devices are seen by torch.\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load a model\n",
    "model = YOLO('yolov8n.pt')  # load a pretrained model (recommended for training)\n",
    "\n",
    "# Train the model with 2 GPUs\n",
    "results = model.train(data='data.yaml', epochs=100, imgsz=640, device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5bf496c-65c0-4bc8-b076-e626ad5d87cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.1.24 üöÄ Python-3.11.6 torch-2.1.2+cu118 CUDA:0 (NVIDIA GeForce RTX 3090, 24576MiB)\n",
      "Model summary (fused): 168 layers, 3006428 parameters, 0 gradients, 8.1 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning E:\\yolov8_env\\ultralytics_android_app\\step_1_train_test_export\\dataset\\val\\labels.cac\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ‚ö†Ô∏è E:\\yolov8_env\\ultralytics_android_app\\step_1_train_test_export\\dataset\\val\\images\\8YGAV61FP6DX_jpg.rf.58da95b585c9603f5f940e73eff64a57.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0132]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ‚ö†Ô∏è E:\\yolov8_env\\ultralytics_android_app\\step_1_train_test_export\\dataset\\val\\images\\CK5FP969OKVT_jpg.rf.39129739a321267395ceb3dc451f0d26.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0011]\n",
      "WARNING ‚ö†Ô∏è Box and segment counts should be equal, but got len(segments) = 20, len(boxes) = 149. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        116        149      0.827       0.53      0.641      0.467\n",
      "                  fork        116         46      0.575      0.652      0.587      0.359\n",
      "                 knife        116          4          1          0      0.278      0.112\n",
      "                 plate        116         11      0.981      0.636      0.859      0.746\n",
      "                 spoon        116         88      0.753      0.832      0.841       0.65\n",
      "Speed: 0.4ms preprocess, 6.9ms inference, 0.0ms loss, 1.0ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\val\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([     0.3591,     0.11213,     0.74554,     0.65032])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Validate model\n",
    "\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Load a model\n",
    "model = YOLO('runs/detect/train3/weights/best.pt')  # load a custom model\n",
    "\n",
    "# Validate the model\n",
    "metrics = model.val()  # no arguments needed, dataset and settings remembered\n",
    "metrics.box.map    # map50-95\n",
    "metrics.box.map50  # map50\n",
    "metrics.box.map75  # map75\n",
    "metrics.box.maps   # a list contains map50-95 of each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28af2e54-9d78-4da5-8fa7-137344fcfd3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/3 E:\\yolov8_env\\ultralytics_android_app\\step_1_train_test_export\\test_images\\a.jpg: 640x640 2 forks, 2 spoons, 20.0ms\n",
      "image 2/3 E:\\yolov8_env\\ultralytics_android_app\\step_1_train_test_export\\test_images\\b.jpg: 640x640 3 forks, 2 spoons, 38.0ms\n",
      "image 3/3 E:\\yolov8_env\\ultralytics_android_app\\step_1_train_test_export\\test_images\\d.jpg: 640x416 2 forks, 2 spoons, 17.0ms\n",
      "Speed: 3.0ms preprocess, 25.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 416)\n",
      "Results saved to \u001b[1mruns\\detect\\predict2\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: None\n",
       " masks: None\n",
       " names: {0: 'fork', 1: 'knife', 2: 'plate', 3: 'spoon'}\n",
       " obb: None\n",
       " orig_img: array([[[210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         ...,\n",
       "         [210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         [210, 210, 210]],\n",
       " \n",
       "        [[210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         ...,\n",
       "         [210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         [210, 210, 210]],\n",
       " \n",
       "        [[210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         ...,\n",
       "         [210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         [210, 210, 210]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[240, 240, 240],\n",
       "         [240, 240, 240],\n",
       "         [240, 240, 240],\n",
       "         ...,\n",
       "         [240, 240, 240],\n",
       "         [240, 240, 240],\n",
       "         [240, 240, 240]],\n",
       " \n",
       "        [[240, 240, 240],\n",
       "         [240, 240, 240],\n",
       "         [240, 240, 240],\n",
       "         ...,\n",
       "         [240, 240, 240],\n",
       "         [240, 240, 240],\n",
       "         [240, 240, 240]],\n",
       " \n",
       "        [[240, 240, 240],\n",
       "         [240, 240, 240],\n",
       "         [240, 240, 240],\n",
       "         ...,\n",
       "         [240, 240, 240],\n",
       "         [240, 240, 240],\n",
       "         [240, 240, 240]]], dtype=uint8)\n",
       " orig_shape: (512, 512)\n",
       " path: 'E:\\\\yolov8_env\\\\ultralytics_android_app\\\\step_1_train_test_export\\\\test_images\\\\a.jpg'\n",
       " probs: None\n",
       " save_dir: 'runs\\\\detect\\\\predict2'\n",
       " speed: {'preprocess': 3.0007362365722656, 'inference': 19.999265670776367, 'postprocess': 2.0003318786621094},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: None\n",
       " masks: None\n",
       " names: {0: 'fork', 1: 'knife', 2: 'plate', 3: 'spoon'}\n",
       " obb: None\n",
       " orig_img: array([[[255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         ...,\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255]],\n",
       " \n",
       "        [[255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         ...,\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255]],\n",
       " \n",
       "        [[255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         ...,\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[254, 254, 254],\n",
       "         [254, 254, 254],\n",
       "         [254, 254, 254],\n",
       "         ...,\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255]],\n",
       " \n",
       "        [[254, 254, 254],\n",
       "         [254, 254, 254],\n",
       "         [254, 254, 254],\n",
       "         ...,\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255]],\n",
       " \n",
       "        [[254, 254, 254],\n",
       "         [254, 254, 254],\n",
       "         [254, 254, 254],\n",
       "         ...,\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255]]], dtype=uint8)\n",
       " orig_shape: (500, 500)\n",
       " path: 'E:\\\\yolov8_env\\\\ultralytics_android_app\\\\step_1_train_test_export\\\\test_images\\\\b.jpg'\n",
       " probs: None\n",
       " save_dir: 'runs\\\\detect\\\\predict2'\n",
       " speed: {'preprocess': 3.000497817993164, 'inference': 37.999629974365234, 'postprocess': 1.9989013671875},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: None\n",
       " masks: None\n",
       " names: {0: 'fork', 1: 'knife', 2: 'plate', 3: 'spoon'}\n",
       " obb: None\n",
       " orig_img: array([[[217, 217, 217],\n",
       "         [235, 235, 235],\n",
       "         [229, 229, 229],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]],\n",
       " \n",
       "        [[231, 231, 231],\n",
       "         [252, 252, 252],\n",
       "         [249, 249, 249],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]],\n",
       " \n",
       "        [[229, 229, 229],\n",
       "         [253, 253, 253],\n",
       "         [254, 254, 254],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         ...,\n",
       "         [252, 252, 252],\n",
       "         [247, 247, 247],\n",
       "         [243, 243, 243]],\n",
       " \n",
       "        [[230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         ...,\n",
       "         [246, 246, 246],\n",
       "         [241, 241, 241],\n",
       "         [238, 238, 238]],\n",
       " \n",
       "        [[230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         ...,\n",
       "         [237, 237, 237],\n",
       "         [231, 231, 231],\n",
       "         [228, 228, 228]]], dtype=uint8)\n",
       " orig_shape: (480, 300)\n",
       " path: 'E:\\\\yolov8_env\\\\ultralytics_android_app\\\\step_1_train_test_export\\\\test_images\\\\d.jpg'\n",
       " probs: None\n",
       " save_dir: 'runs\\\\detect\\\\predict2'\n",
       " speed: {'preprocess': 2.9969215393066406, 'inference': 17.001628875732422, 'postprocess': 1.9989013671875}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  Prediction using trained model\n",
    "\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Load a pretrained YOLOv8n model\n",
    "model = YOLO('runs/detect/train3/weights/best.pt')\n",
    "\n",
    "# Run inference \n",
    "model.predict('test_images', save=True, imgsz=640, conf=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3d815e-d7ad-4f2d-b540-b3039f1230e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.1.24 üöÄ Python-3.11.6 torch-2.1.2+cu118 CPU (AMD Ryzen 9 5900X 12-Core Processor)\n",
      "Model summary (fused): 168 layers, 3006428 parameters, 0 gradients, 8.1 GFLOPs\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'runs\\detect\\train3\\weights\\best.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 8, 8400) (6.0 MB)\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirement ['onnxruntime-gpu'] not found, attempting AutoUpdate...\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m ‚ùå Command 'pip install --no-cache \"onnxruntime-gpu\" --extra-index-url https://pypi.ngc.nvidia.com' returned non-zero exit status 1.\n",
      "\n",
      "\u001b[34m\u001b[1mTensorFlow SavedModel:\u001b[0m starting export with tensorflow 2.13.1...\n",
      "Downloading https://github.com/ultralytics/assets/releases/download/v8.1.0/calibration_image_sample_data_20x128x128x3_float32.npy.zip to 'calibration_image_sample_data_20x128x128x3_float32.npy.zip'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.11M/1.11M [00:00<00:00, 11.2MB/s]\n",
      "Unzipping calibration_image_sample_data_20x128x128x3_float32.npy.zip to E:\\yolov8_env\\ultralytics_a"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirement ['onnxruntime-gpu'] not found, attempting AutoUpdate...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m\u001b[1mrequirements:\u001b[0m ‚ùå Command 'pip install --no-cache \"onnxruntime-gpu\" ' returned non-zero exit status 1.\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.16.0 opset 17...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m simplifying with onnxsim 0.4.36...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success ‚úÖ 15.1s, saved as 'runs\\detect\\train3\\weights\\best.onnx' (11.7 MB)\n",
      "\u001b[34m\u001b[1mTensorFlow SavedModel:\u001b[0m starting TFLite export with onnx2tf 1.17.5...\n",
      "\u001b[34m\u001b[1mTensorFlow SavedModel:\u001b[0m export success ‚úÖ 43.0s, saved as 'runs\\detect\\train3\\weights\\best_saved_model' (29.5 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mTensorFlow Lite:\u001b[0m starting export with tensorflow 2.13.1...\n",
      "\u001b[34m\u001b[1mTensorFlow Lite:\u001b[0m export success ‚úÖ 0.0s, saved as 'runs\\detect\\train3\\weights\\best_saved_model\\best_float32.tflite' (11.7 MB)\n",
      "\n",
      "Export complete (44.7s)\n",
      "Results saved to \u001b[1mE:\\yolov8_env\\ultralytics_android_app\\step_1_train_test_export\\runs\\detect\\train3\\weights\u001b[0m\n",
      "Predict:         yolo predict task=detect model=runs\\detect\\train3\\weights\\best_saved_model\\best_float32.tflite imgsz=640  \n",
      "Validate:        yolo val task=detect model=runs\\detect\\train3\\weights\\best_saved_model\\best_float32.tflite imgsz=640 data=data.yaml  \n",
      "Visualize:       https://netron.app\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'runs\\\\detect\\\\train3\\\\weights\\\\best_saved_model\\\\best_float32.tflite'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  Export model to tflite\n",
    "\n",
    "\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Load a model\n",
    "model = YOLO('runs/detect/train3/weights/best.pt')  # load a custom trained model\n",
    "\n",
    "# Export the model\n",
    "model.export(format='tflite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3aae33b-b14a-43f6-861f-3e0f2d21407f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading runs\\detect\\train3\\weights\\best_saved_model\\best_float32.tflite for TensorFlow Lite inference...\n",
      "\n",
      "image 1/1 E:\\yolov8_env\\ultralytics_android_app\\step_1_train_test_export\\test_images\\b.jpg: 640x640 3 forks, 2 spoons, 184.0ms\n",
      "Speed: 3.0ms preprocess, 184.0ms inference, 52.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\predict\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: None\n",
       " masks: None\n",
       " names: {0: 'fork', 1: 'knife', 2: 'plate', 3: 'spoon'}\n",
       " obb: None\n",
       " orig_img: array([[[255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         ...,\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255]],\n",
       " \n",
       "        [[255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         ...,\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255]],\n",
       " \n",
       "        [[255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         ...,\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[254, 254, 254],\n",
       "         [254, 254, 254],\n",
       "         [254, 254, 254],\n",
       "         ...,\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255]],\n",
       " \n",
       "        [[254, 254, 254],\n",
       "         [254, 254, 254],\n",
       "         [254, 254, 254],\n",
       "         ...,\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255]],\n",
       " \n",
       "        [[254, 254, 254],\n",
       "         [254, 254, 254],\n",
       "         [254, 254, 254],\n",
       "         ...,\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255]]], dtype=uint8)\n",
       " orig_shape: (500, 500)\n",
       " path: 'E:\\\\yolov8_env\\\\ultralytics_android_app\\\\step_1_train_test_export\\\\test_images\\\\b.jpg'\n",
       " probs: None\n",
       " save_dir: 'runs\\\\detect\\\\predict'\n",
       " speed: {'preprocess': 3.0007362365722656, 'inference': 183.99810791015625, 'postprocess': 52.13308334350586}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prediction using custom tflite model\n",
    "\n",
    "#  Prediction using trained model\n",
    "\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Load a pretrained YOLOv8n model\n",
    "model = YOLO('runs\\\\detect\\\\train3\\\\weights\\\\best_saved_model\\\\best_float32.tflite')\n",
    "\n",
    "# Run inference \n",
    "model.predict('test_images', save=True, imgsz=640, conf=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6340d01d-6c54-4989-9f04-91206888e37a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
